{
  "nodes": [
    {
      "nodeId": "userGuide",
      "name": "common:core.module.template.system_config",
      "intro": "common:core.module.template.system_config_info",
      "avatar": "core/workflow/template/systemConfig",
      "flowNodeType": "userGuide",
      "position": {
        "x": 49.137233881709335,
        "y": -445.7446492944009
      },
      "version": "481",
      "inputs": [
        {
          "key": "welcomeText",
          "renderTypeList": [
            "hidden"
          ],
          "valueType": "string",
          "label": "core.app.Welcome Text",
          "value": ""
        },
        {
          "key": "variables",
          "renderTypeList": [
            "hidden"
          ],
          "valueType": "any",
          "label": "core.app.Chat Variable",
          "value": []
        },
        {
          "key": "questionGuide",
          "valueType": "any",
          "renderTypeList": [
            "hidden"
          ],
          "label": "core.app.Question Guide",
          "value": {
            "open": false
          }
        },
        {
          "key": "tts",
          "renderTypeList": [
            "hidden"
          ],
          "valueType": "any",
          "label": "",
          "value": {
            "type": "web"
          }
        },
        {
          "key": "whisper",
          "renderTypeList": [
            "hidden"
          ],
          "valueType": "any",
          "label": "",
          "value": {
            "open": false,
            "autoSend": false,
            "autoTTSResponse": false
          }
        },
        {
          "key": "scheduleTrigger",
          "renderTypeList": [
            "hidden"
          ],
          "valueType": "any",
          "label": "",
          "value": null
        }
      ],
      "outputs": []
    },
    {
      "nodeId": "448745",
      "name": "common:core.module.template.work_start",
      "intro": "",
      "avatar": "core/workflow/template/workflowStart",
      "flowNodeType": "workflowStart",
      "position": {
        "x": 572.4550321443911,
        "y": -507.8421976814977
      },
      "version": "481",
      "inputs": [
        {
          "key": "userChatInput",
          "renderTypeList": [
            "reference",
            "textarea"
          ],
          "valueType": "string",
          "label": "workflow:user_question",
          "required": true,
          "toolDescription": "用户问题",
          "debugLabel": ""
        }
      ],
      "outputs": [
        {
          "id": "userChatInput",
          "key": "userChatInput",
          "label": "common:core.module.input.label.user question",
          "type": "static",
          "valueType": "string",
          "description": ""
        }
      ]
    },
    {
      "nodeId": "bBVKC7SO6bLzLceA",
      "name": "AI 对话",
      "intro": "AI 大模型对话",
      "avatar": "core/workflow/template/aiChat",
      "flowNodeType": "chatNode",
      "showStatus": true,
      "position": {
        "x": 1293.7616626843408,
        "y": -713.8660239722025
      },
      "version": "4.9.7",
      "inputs": [
        {
          "key": "model",
          "renderTypeList": [
            "settingLLMModel",
            "reference"
          ],
          "label": "common:core.module.input.label.aiModel",
          "valueType": "string",
          "value": "deepseek-v3.1",
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "temperature",
          "renderTypeList": [
            "hidden"
          ],
          "label": "",
          "valueType": "number",
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "maxToken",
          "renderTypeList": [
            "hidden"
          ],
          "label": "",
          "valueType": "number",
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "isResponseAnswerText",
          "renderTypeList": [
            "hidden"
          ],
          "label": "",
          "value": false,
          "valueType": "boolean",
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "aiChatQuoteRole",
          "renderTypeList": [
            "hidden"
          ],
          "label": "",
          "valueType": "string",
          "value": "system",
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "quoteTemplate",
          "renderTypeList": [
            "hidden"
          ],
          "label": "",
          "valueType": "string",
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "quotePrompt",
          "renderTypeList": [
            "hidden"
          ],
          "label": "",
          "valueType": "string",
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "aiChatVision",
          "renderTypeList": [
            "hidden"
          ],
          "label": "",
          "valueType": "boolean",
          "value": false,
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "aiChatReasoning",
          "renderTypeList": [
            "hidden"
          ],
          "label": "",
          "valueType": "boolean",
          "value": false,
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "aiChatTopP",
          "renderTypeList": [
            "hidden"
          ],
          "label": "",
          "valueType": "number",
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "aiChatStopSign",
          "renderTypeList": [
            "hidden"
          ],
          "label": "",
          "valueType": "string",
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "aiChatResponseFormat",
          "renderTypeList": [
            "hidden"
          ],
          "label": "",
          "valueType": "string",
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "aiChatJsonSchema",
          "renderTypeList": [
            "hidden"
          ],
          "label": "",
          "valueType": "string",
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "systemPrompt",
          "renderTypeList": [
            "textarea",
            "reference"
          ],
          "max": 3000,
          "valueType": "string",
          "label": "common:core.ai.Prompt",
          "description": "common:core.app.tip.systemPromptTip",
          "placeholder": "common:core.app.tip.chatNodeSystemPromptTip",
          "value": "Role: 学术论文分析专家\n\nBackground: 你是一位专业的学术研究助理，专门帮助研究人员系统化地分析和总结学术论文的核心内容。你具备深厚的学术背景和严谨的分析能力，能够准确提取论文关键信息并按照标准学术格式进行结构化输出。\n\nAttention: 专注于理解论文的深层逻辑和学术价值，确保每个分析模块都准确反映原文内容，同时保持学术严谨性和客观性。你需要归纳出：\n动机 (Motivation)\n背景 (Background)\n同类方法的缺陷 (Limitations of Existing Methods)\n解决的问题 (Problem Solved)\n方法 (Methodology)\n实验 (Experiments)\n结论 (Conclusion)\n\n\nSkills:\n1. 精准识别论文的研究动机和问题意识\n2. 深入理解研究背景和相关工作的发展脉络\n3. 客观分析现有方法的局限性并指出改进空间\n4. 清晰阐述论文解决的具体问题和创新点\n5. 系统梳理研究方法论和技术路线的逻辑结构\n\nGoals:\n1. 完整呈现论文的研究动机和问题提出过程\n2. 准确描述研究背景和领域现状\n3. 客观评价现有方法的优缺点和局限性\n4. 明确阐述本文解决的核心问题和贡献\n5. 系统总结研究方法和实验验证过程\n\nConstrains:\n1. 严格基于论文原文内容进行分析，不添加主观臆测\n2. 保持学术客观性，避免过度夸大或贬低研究成果\n3. 确保每个模块内容相互独立又逻辑连贯\n4. 使用规范的学术语言和术语表达\n5. 遵循学术伦理，准确引用和归纳原文内容\n\nOutputFormat:\n1. 每个模块使用明确的标题标识，内容层次清晰\n2. 使用学术规范的表达方式，避免口语化表述\n3. 关键术语和概念保持与原文一致，确保准确性\n4. 输出md，包含：\n动机 (Motivation)\n背景 (Background)\n同类方法的缺陷 (Limitations of Existing Methods)\n解决的问题 (Problem Solved)\n方法 (Methodology)\n实验 (Experiments)\n结论 (Conclusion)\n\n要求：注意输出尽可能详尽\n至少8000字以上",
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "history",
          "renderTypeList": [
            "numberInput",
            "reference"
          ],
          "valueType": "chatHistory",
          "label": "common:core.module.input.label.chat history",
          "description": "workflow:max_dialog_rounds",
          "required": true,
          "min": 0,
          "max": 50,
          "value": 6,
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "quoteQA",
          "renderTypeList": [
            "settingDatasetQuotePrompt"
          ],
          "label": "",
          "debugLabel": "知识库引用",
          "valueType": "datasetQuote",
          "description": "",
          "toolDescription": "",
          "value": [
            "u2dnJcixIb4Akk4U",
            "quoteQA"
          ]
        },
        {
          "key": "fileUrlList",
          "renderTypeList": [
            "reference",
            "input"
          ],
          "label": "app:workflow.user_file_input",
          "debugLabel": "文件链接",
          "description": "app:workflow.user_file_input_desc",
          "valueType": "arrayString",
          "value": [
            [
              "448745",
              "userFiles"
            ]
          ],
          "toolDescription": ""
        },
        {
          "key": "userChatInput",
          "renderTypeList": [
            "reference",
            "textarea"
          ],
          "valueType": "string",
          "label": "workflow:user_question",
          "toolDescription": "用户问题",
          "required": true,
          "value": [
            "VARIABLE_NODE_ID",
            "tKEUT9iQ"
          ],
          "selectedTypeIndex": 0,
          "debugLabel": ""
        }
      ],
      "outputs": [
        {
          "id": "history",
          "key": "history",
          "required": true,
          "label": "common:core.module.output.label.New context",
          "description": "将本次回复内容拼接上历史记录，作为新的上下文返回",
          "valueType": "chatHistory",
          "valueDesc": "{\n  obj: System | Human | AI;\n  value: string;\n}[]",
          "type": "static"
        },
        {
          "id": "answerText",
          "key": "answerText",
          "required": true,
          "label": "common:core.module.output.label.Ai response content",
          "description": "将在 stream 回复完毕后触发",
          "valueType": "string",
          "type": "static",
          "valueDesc": ""
        },
        {
          "id": "reasoningText",
          "key": "reasoningText",
          "required": false,
          "label": "workflow:reasoning_text",
          "valueType": "string",
          "type": "static",
          "invalid": false,
          "valueDesc": "",
          "description": ""
        },
        {
          "id": "system_error_text",
          "key": "system_error_text",
          "type": "error",
          "valueType": "string",
          "label": "workflow:error_text",
          "valueDesc": "",
          "description": ""
        }
      ],
      "catchError": false
    },
    {
      "nodeId": "mmgcQ1sr2n1jdRWV",
      "name": "AI 对话#2",
      "intro": "AI 大模型对话",
      "avatar": "core/workflow/template/aiChat",
      "flowNodeType": "chatNode",
      "showStatus": true,
      "position": {
        "x": 2192.22389031277,
        "y": 76.0005398843607
      },
      "version": "4.9.7",
      "inputs": [
        {
          "key": "model",
          "renderTypeList": [
            "settingLLMModel",
            "reference"
          ],
          "label": "common:core.module.input.label.aiModel",
          "valueType": "string",
          "value": "doubao-seed-1-6-250615",
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "temperature",
          "renderTypeList": [
            "hidden"
          ],
          "label": "",
          "valueType": "number",
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "maxToken",
          "renderTypeList": [
            "hidden"
          ],
          "label": "",
          "valueType": "number",
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "isResponseAnswerText",
          "renderTypeList": [
            "hidden"
          ],
          "label": "",
          "value": false,
          "valueType": "boolean",
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "aiChatQuoteRole",
          "renderTypeList": [
            "hidden"
          ],
          "label": "",
          "valueType": "string",
          "value": "system",
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "quoteTemplate",
          "renderTypeList": [
            "hidden"
          ],
          "label": "",
          "valueType": "string",
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "quotePrompt",
          "renderTypeList": [
            "hidden"
          ],
          "label": "",
          "valueType": "string",
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "aiChatVision",
          "renderTypeList": [
            "hidden"
          ],
          "label": "",
          "valueType": "boolean",
          "value": false,
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "aiChatReasoning",
          "renderTypeList": [
            "hidden"
          ],
          "label": "",
          "valueType": "boolean",
          "value": false,
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "aiChatTopP",
          "renderTypeList": [
            "hidden"
          ],
          "label": "",
          "valueType": "number",
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "aiChatStopSign",
          "renderTypeList": [
            "hidden"
          ],
          "label": "",
          "valueType": "string",
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "aiChatResponseFormat",
          "renderTypeList": [
            "hidden"
          ],
          "label": "",
          "valueType": "string",
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "aiChatJsonSchema",
          "renderTypeList": [
            "hidden"
          ],
          "label": "",
          "valueType": "string",
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "systemPrompt",
          "renderTypeList": [
            "textarea",
            "reference"
          ],
          "max": 3000,
          "valueType": "string",
          "label": "common:core.ai.Prompt",
          "description": "common:core.app.tip.systemPromptTip",
          "placeholder": "common:core.app.tip.chatNodeSystemPromptTip",
          "value": "请将对论文代码的分析、对论文进行外部知识库检索的补充知识，\n补充到现有的论文分析中，保留‘现有的论文分析’的结构不变，补充内容需要尽量详细即：\n动机 (Motivation)\n背景 (Background)\n同类方法的缺陷 (Limitations of Existing Methods)\n解决的问题 (Problem Solved)\n方法 (Methodology)\n实验 (Experiments)\n结论 (Conclusion)\n\n注意：\n1.对于“对论文代码的分析”中的图表、伪代码 、公式、流程等，都可以加入到‘现有的论文分析’中，不可遗漏和丢弃。对于模型架构图，可以放在方法部分的最开始。对于核心算法的分析可以放到相关方法的介绍中。对于有mermaid格式的，保留mermaid格式。\n2.对于“对论文进行外部知识库检索的补充知识”中出现的个人观点理解，请增加引用。\n\n需要的地方可附上类似以下输出模块\n\n**代码实现细节示例**\n实现位置:attention tfpy:77-82attention keras.py:103-119输入参数: Q, K,V(shape[batch size, seq len, d k])处理步骤:\n1.计算点积:`A =tf.matmul(Q, Ktranspose b=True)2.缩放:A = A/tf.sqrt(float(size_per head))3.应用掩码:`A = Mask(A,V len,mode='add')`4.Softmax:`A =tfnn.softmax(A)5.加权求和:O = tfmatmul(A, V)\n\n记住：方法部分每段都附上相应代码。\n\n非方法部分可以用伪代码，也可以用流程图",
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "history",
          "renderTypeList": [
            "numberInput",
            "reference"
          ],
          "valueType": "chatHistory",
          "label": "common:core.module.input.label.chat history",
          "description": "workflow:max_dialog_rounds",
          "required": true,
          "min": 0,
          "max": 50,
          "value": 6,
          "debugLabel": "",
          "toolDescription": ""
        },
        {
          "key": "quoteQA",
          "renderTypeList": [
            "settingDatasetQuotePrompt"
          ],
          "label": "",
          "debugLabel": "知识库引用",
          "valueType": "datasetQuote",
          "description": "",
          "toolDescription": ""
        },
        {
          "key": "fileUrlList",
          "renderTypeList": [
            "reference",
            "input"
          ],
          "label": "app:workflow.user_file_input",
          "debugLabel": "文件链接",
          "description": "app:workflow.user_file_input_desc",
          "valueType": "arrayString",
          "value": [
            [
              "448745",
              "userFiles"
            ]
          ],
          "toolDescription": ""
        },
        {
          "key": "userChatInput",
          "renderTypeList": [
            "reference",
            "textarea"
          ],
          "valueType": "string",
          "label": "workflow:user_question",
          "toolDescription": "用户问题",
          "required": true,
          "value": "【现有的论文分析】\n{{$bBVKC7SO6bLzLceA.answerText$}}\n\n【对论文代码的分析】\n{{$VARIABLE_NODE_ID.gKxpZiRI$}}\n\n【对论文进行外部知识库检索的补充知识】\n{{$VARIABLE_NODE_ID.ocN5KV4O$}}",
          "selectedTypeIndex": 1,
          "debugLabel": ""
        }
      ],
      "outputs": [
        {
          "id": "history",
          "key": "history",
          "required": true,
          "label": "common:core.module.output.label.New context",
          "description": "将本次回复内容拼接上历史记录，作为新的上下文返回",
          "valueType": "chatHistory",
          "valueDesc": "{\n  obj: System | Human | AI;\n  value: string;\n}[]",
          "type": "static"
        },
        {
          "id": "answerText",
          "key": "answerText",
          "required": true,
          "label": "common:core.module.output.label.Ai response content",
          "description": "将在 stream 回复完毕后触发",
          "valueType": "string",
          "type": "static",
          "valueDesc": ""
        },
        {
          "id": "reasoningText",
          "key": "reasoningText",
          "required": false,
          "label": "workflow:reasoning_text",
          "valueType": "string",
          "type": "static",
          "invalid": false,
          "valueDesc": "",
          "description": ""
        },
        {
          "id": "system_error_text",
          "key": "system_error_text",
          "type": "error",
          "valueType": "string",
          "label": "workflow:error_text",
          "valueDesc": "",
          "description": ""
        }
      ],
      "catchError": false
    },
    {
      "nodeId": "bO0PXQJqyaDgVwsC",
      "name": "指定回复",
      "intro": "该模块可以直接回复一段指定的内容。常用于引导、提示。非字符串内容传入时，会转成字符串进行输出。",
      "avatar": "core/workflow/template/reply",
      "flowNodeType": "answerNode",
      "position": {
        "x": 3126.4404752153614,
        "y": 424.85708450677646
      },
      "inputs": [
        {
          "key": "text",
          "renderTypeList": [
            "textarea",
            "reference"
          ],
          "valueType": "any",
          "required": true,
          "label": "common:core.module.input.label.Response content",
          "description": "common:core.module.input.description.Response content",
          "placeholder": "common:core.module.input.description.Response content",
          "value": "{{$mmgcQ1sr2n1jdRWV.answerText$}}",
          "debugLabel": "",
          "toolDescription": ""
        }
      ],
      "outputs": []
    }
  ],
  "edges": [
    {
      "source": "bBVKC7SO6bLzLceA",
      "target": "mmgcQ1sr2n1jdRWV",
      "sourceHandle": "bBVKC7SO6bLzLceA-source-right",
      "targetHandle": "mmgcQ1sr2n1jdRWV-target-left"
    },
    {
      "source": "mmgcQ1sr2n1jdRWV",
      "target": "bO0PXQJqyaDgVwsC",
      "sourceHandle": "mmgcQ1sr2n1jdRWV-source-right",
      "targetHandle": "bO0PXQJqyaDgVwsC-target-left"
    },
    {
      "source": "448745",
      "target": "bBVKC7SO6bLzLceA",
      "sourceHandle": "448745-source-right",
      "targetHandle": "bBVKC7SO6bLzLceA-target-left"
    }
  ],
  "chatConfig": {
    "variables": [
      {
        "key": "tKEUT9iQ",
        "label": "paper_origin",
        "type": "input",
        "description": "",
        "required": true,
        "valueType": "string",
        "defaultValue": "% CVPR 2022 Paper Template\n% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)\n% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)\n\n\\documentclass[10pt,twocolumn,letterpaper]{article}\n\n%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION\n%\\usepackage[review]{cvpr}      % To produce the REVIEW version\n\\usepackage{cvpr}              % To produce the CAMERA-READY version\n%\\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version\n\n% Include other packages here, before hyperref.\n\\usepackage{graphicx}\n\\usepackage{amsmath}\n\\usepackage{amssymb}\n\\usepackage{booktabs}\n\\usepackage{makecell}\n\\usepackage{multirow}\n\\usepackage{xcolor}\n\n\\usepackage{comment}\n% It is strongly recommended to use hyperref, especially for the review version.\n% hyperref with option pagebackref eases the reviewers' job.\n% Please disable hyperref *only* if you encounter grave issues, e.g. with the\n% file validation for the camera-ready version.\n%\n% If you comment hyperref and then uncomment it, you should delete\n% ReviewTempalte.aux before re-running LaTeX.\n% (Or just hit 'q' on the first LaTeX run, let it finish, and you\n%  should be clear).\n\\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}\n\n% Support for easy cross-referencing\n\\usepackage[capitalize]{cleveref}\n\\crefname{section}{Sec.}{Secs.}\n\\Crefname{section}{Section}{Sections}\n\\Crefname{table}{Table}{Tables}\n\\crefname{table}{Tab.}{Tabs.}\n\n\n%%%%%%%%% PAPER ID  - PLEASE UPDATE\n\\def\\confName{CVPR}\n\\def\\confYear{2022}\n\n\\definecolor{Highlight}{HTML}{39b54a}  % green\n\\newcommand{\\shl}[1]{\\footnotesize{#1}}\n%\\newcommand{\\hl}[1]{\\footnotesize{\\textcolor{Highlight}{#1}}}\n\\newcommand{\\hl}[1]{\\footnotesize{#1}}\n\n\\begin{document}\n\n%%%%%%%%% TITLE - PLEASE UPDATE\n\\title{Swin Transformer V2: Scaling Up Capacity and Resolution}\n\n%Continuous Position Bias and Scaling up}\n\n\\author{\nZe Liu\\thanks{Equal. $^\\dag$Project lead. Ze, Yutong, Zhuliang, Zhenda, Yixuan, Jia are long-term interns at MSRA.}\n\\quad Han Hu\\textsuperscript{*$\\dag$} \\quad Yutong Lin \\quad Zhuliang Yao \\quad Zhenda Xie\n\\quad Yixuan Wei \\quad Jia Ning  \\\\\n\\quad Yue Cao \\quad Zheng Zhang\n\\quad Li Dong \\quad Furu Wei \\quad Baining Guo \\\\\n{Microsoft Research Asia}\\\\\n\\small{\\texttt{\\{v-zeliu1,hanhu,t-yutonglin,t-zhuyao,t-zhxie,t-yixuanwei,v-jianing\\}@microsoft.com}} \\\\\n\\small{\\texttt{\\{yuecao,zhez,lidong1,fuwei,bainguo\\}@microsoft.com}}\n}\n\n\\maketitle\n\n%%%%%%%%% ABSTRACT\n\\begin{abstract}\n\nLarge-scale NLP models have been shown to significantly improve the performance on language tasks with no signs of saturation. They also demonstrate amazing few-shot capabilities like that of human beings. This paper aims to explore large-scale models in computer vision. We tackle three major issues in training and application of large vision models, including training instability, resolution gaps between pre-training and fine-tuning, and hunger on labelled data. Three main techniques are proposed: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) A log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) A self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images. Through these techniques, this paper successfully trained a 3 billion-parameter Swin Transformer V2 model, which is the largest dense vision model to date, and makes it capable of training with images of up to 1,536$\\times$1,536 resolution. It set new performance records on 4 representative vision tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification. Also note our training is much more efficient than that in Google's billion-level visual models, which consumes 40 times less labelled data and 40 times less training time. Code is available at \\url{https://github.com/microsoft/Swin-Transformer}.\n\n\\end{abstract}\n\n%%%%%%%%% BODY TEXT\n\\section{Introduction}\n\\label{sec:intro}\n\nScaling up language models has been incredibly successful. It significantly improves a model’s performance on language tasks~\\cite{devlin2018bert,radford2019language,raffel2019t5,Turing-17B,fedus2021switch,Megatron-Turing-530B} and the model demonstrates amazing few-shot capabilities similar to that of human beings~\\cite{brown2020language}. Since the BERT large model with 340 million parameters~\\cite{devlin2018bert}, language models are quickly scaled up by more than 1,000 times in a few years, reaching 530 billion dense parameters~\\cite{Megatron-Turing-530B} and 1.6 trillion sparse parameters~\\cite{fedus2021switch}. These large language models are also found to possess increasingly strong few-shot capabilities akin to human intelligence for a broad range of language tasks~\\cite{brown2020language}. \n\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{figs/teaserv6.pdf}\n    \\caption{To better scale up model capacity and window resolution, several adaptions are made on the original Swin Transformer architecture (V1): 1) A \\emph{res-post-norm} to replace the previous \\emph{pre-norm} configuration; 2) A \\emph{scaled cosine attention} to replace the original \\emph{dot product attention}; 3) A \\emph{log-spaced continuous} relative position bias approach to replace the previous \\emph{parameterized} approach. Adaptions 1) and 2) make it easier for the model to scale up capacity. Adaption 3) makes the model to be transferred more effectively across window resolutions. The adapted architecture is named Swin Transformer V2.}\n    \\label{fig:teaser}\n    \\vspace{-0.5em}\n\\end{figure}\n\n\nOn the other hand, the scaling up of vision models has been lagging behind. While it has long been recognized that larger vision models usually perform better on vision tasks~\\cite{simonyan2014vgg,he2015resnet}, the absolute model size was just able to reach about 1-2 billion parameters very recently~\\cite{kolesnikov2019bigtransfer,goyal2021selfsupervised,zhai2021scaling,riquelme2021scaling,dai2021coatnet}. More importantly, unlike large language models, the existing large vision models are applied to the image classification task only~\\cite{zhai2021scaling,riquelme2021scaling,dai2021coatnet}. \n\nTo successfully train large and general vision model, we need to address a few key issues. Firstly, our experiments with large vision models reveal an instability issue in training. We find that the discrepancy of activation amplitudes across layers becomes significantly greater in large models. A closer look at the original architecture reveals that this is caused by the output of the residual unit directly added back to the main branch. The result is that the activation values are accumulated layer by layer, and the amplitudes at deeper layers are thus significantly larger than those at early layers. To address this issue, we propose a new normalization configuration, called res-post-norm, which moves the LN layer from the beginning of each residual unit to the backend, as shown in Figure~\\ref{fig:teaser}. We find this new configuration produces much milder activation values across the network layers. We also propose a scaled cosine attention to replace the previous dot product attention. The scaled cosine attention makes the computation irrelevant to amplitudes of block inputs, and the attention values are less likely to fall into extremes. In our experiments, the proposed two techniques not only make the training process more stable but also improve the accuracy especially for larger models. \n\n\nSecondly, many downstream vision tasks such as object detection and semantic segmentation require high resolution input images or large attention windows. The window size variations between low-resolution pre-training and high-resolution fine-tuning can be quite large. The current common practice is to perform a bi-cubic interpolation of the position bias maps~\\cite{dosovitskiy2020vit,liu2021swin}. This simple fix is somewhat ad-hoc and the result is usually sub-optimal. We introduce a log-spaced continuous position bias (Log-CPB), which generates bias values for arbitrary coordinate ranges by applying a small meta network on the log-spaced coordinate inputs. Since the meta network takes any coordinates, a pre-trained model will be able to freely transfer across window sizes by sharing weights of the meta network. A critical design of our approach is to transform the coordinates into the log-space so that the extrapolation ratio can be low even when the target window size is significantly larger than that of pre-training. \nThe scaling up of model capacity and resolution also leads to prohibitively high GPU memory consumption with existing vision models. To resolve the memory issue, we incorporate several important techniques including zero-optimizer~\\cite{rajbhandari2020zero}, activation check pointing~\\cite{chen2016training} and a novel implementation of sequential self-attention computation. With these techniques, the GPU memory consumption of large models and resolutions is significantly reduced with only marginal effect on the training speed. \n\nWith the above techniques, we successfully trained a 3 billion Swin Transformer model and effectively transferred it to various vision tasks with image resolution as large as 1,536$\\times$1,536, using Nvidia A100-40G GPUs. In our model pre-training, we also employ self-supervised pre-training to reduce the dependency on super-huge labeled data. With 40$\\times$ less labelled data than that in previous practice (JFT-3B), the 3 billion model achieves the state-of-the-art accuracy on a broad range of vision benchmarks. Specifically, it obtains 84.0\\% top-1 accuracy on the ImageNet-V2 image classification validation set~\\cite{recht2019imagenet}, 63.1 / 54.4 box / mask AP on the COCO test-dev set of object detection, 59.9 mIoU on ADE20K semantic segmentation, and 86.8\\% top-1 accuracy on Kinetics-400 video action classification, which are +NA\\%, +4.4/+3.3, +6.3 and +1.9 higher than the best numbers in the original Swin Transformers~\\cite{liu2021swin,liu2021video}, and surpass previous best records by +0.8\\% (\\cite{zhai2021scaling}), +1.8/+1.4~(\\cite{xu2021endtoend}), +1.5 (\\cite{bao2021beit}) and +1.4\\% (\\cite{ryoo2021tokenlearner}).\n\nBy scaling up both capacity and resolution of vision models with strong performance on general vision tasks, just like a good language model’s performance on general NLP tasks, we aim to stimulate more research in this direction so that we can eventually close the capacity gap between vision and language models and facilitate the joint modeling of the two domains.\n\n\\section{Related Works}\n\n\\paragraph{Language networks and scaling up} Transformer has served the standard network since the pioneer work of~\\cite{vaswani2017attention}. The exploration of scaling this architecture has since begun, and the progress has been accelerated by the invention of effective self-supervised learning approaches, such as masked or auto-regressive language modeling~\\cite{devlin2018bert,radford2019language}, and has been further encouraged by the discovery of a scaling law~\\cite{kaplan2020scaling}. Since then, the capacity of language models has increased dramatically by more than 1,000 times in a few years, from BERT-340M to the Megatron-Turing-530B~\\cite{raffel2019t5,Turing-17B,brown2020language,Megatron-Turing-530B} and sparse Switch-Transformer-1.6T~\\cite{fedus2021switch}. With increased capacity, the accuracy of various language benchmarks has been significantly improved. The zero-shot or few-shot performance is also significantly improved~\\cite{brown2020language}, which is a foundation of human generic intelligence.\n\n\\paragraph{Vision networks and scaling up} CNNs have long been the standard computer vision networks~\\cite{lecun1998lenet, krizhevsky2012alexnet}. Since AlexNet~\\cite{krizhevsky2012alexnet}, architectures have become deeper and larger, which has greatly advanced various visual tasks and largely fueled the wave of deep learning in computer vision, such as VGG~\\cite{simonyan2014vgg}, GoogleNet~\\cite{szegedy2015googlenet} and ResNet~\\ cite{he2015resnet}. In the past two years, the CNN architectures have been further scaled up to about 1 billion parameters ~\\cite{kolesnikov2019bigtransfer, goyal2021selfsupervised}, however, absolute performance may not be so encouraging, perhaps due to inductive biases in the CNN architecture limiting modeling power. \n\nLast year, Transformers started taking over one representative visual benchmark after another, including ImageNet-1K image-level classification benchmarks~\\cite{dosovitskiy2020vit}, COCO region-level object detection benchmark~\\cite{liu2021swin}, ADE20K pixel-level semantic segmentation benchmark~\\cite{zheng2020SETR, liu2021swin}, Kinetics-400 video action classification benchmark~\\cite{arnab2021vivit}, etc. Since these works, numerous vision Transformer variants have been proposed to improve the accuracy at relatively small scale~\\cite{touvron2020deit,li2021localvit,chu2021twins,wang2021pyramid,yuan2021tokenstotoken,zhang2021multiscale,dong2021cswin,yang2021focal,huang2021shuffle,xiao2021early,yuan2021volo}. \nOnly a few works have attempted to scale up the vision Transformers~\\cite{zhai2021scaling,riquelme2021scaling,dai2021coatnet}. However, they rely on a huge image dataset with classification labels, i.e., JFT-3B, and are only applied to image classification problems.\n\n\n\\paragraph{Transferring across window / kernel resolution} For CNNs, previous works typically fixed kernel size during pre-training and fine-tuning. Global vision Transformers, such as ViT~\\cite{dosovitskiy2020vit}, compute attention globally, with the equivalent attention window size linearly proportional to the increased input image resolution. For local vision Transformer architectures, such as Swin Transformer~\\cite{liu2021swin}, the window size can be either fixed or changed during fine-tuning. Allowing variable window sizes is more convenient in use, so as to be divisible by the probably variable entire feature map and to tune receptive fields for better accuracy. To handle the variable window sizes between pre-training and fine-tuning, bi-cubic interpolation was the previous common practice~\\cite{dosovitskiy2020vit, liu2021swin}. In this paper, we propose a log-spaced continuous position bias approach (Log-CPB) that more smoothly transfers pre-trained model weights at low resolution to deal-with higher resolution windows. \n\n\\paragraph{Study on bias terms} In NLP, the relative position bias method proved beneficial~\\cite{raffel2019t5}, compared to the absolute position embedding used in the original Transformer~\\cite{vaswani2017attention}. In computer vision, the relative positional bias method is more commonly used~\\cite{hu2019localrelation, liu2021swin, yang2021focal}, probably because the spatial relationships of visual signals play a more important role in visual modeling. A common practice is to directly learn the bias values as model weights. There are also a few works particularly study how to set and learn the bias terms~\\cite{ke2021rethinking, wu2021rethinking}. \n\n\\paragraph{Continuous convolution and variants} Our Log-CPB approach is also related to earlier works on continuous convolution and variants ~\\cite{schutt2017schnet,wang2018continuousconvcvpr,hu2018relation,liu2020closer}, which utilize a meta network to handle irregular data points. Our Log-CPB approach is inspired by these efforts while solving a different problem of transferring relative position biases in vision Transformers across arbitrary window sizes. We also propose log-spaced coordinates to alleviate the difficulty of extrapolation when transferring between large size changes.\n\n\\section{Swin Transformer V2}\n\n\\subsection{A Brief Review of Swin Transformer}\n\n\\label{sec.swin_v1}\n\nSwin Transformer is a general-purpose computer vision backbone that has achieved strong performance in various granular recognition tasks such as region-level object detection, pixel-level semantic segmentation, and image-level image classification. The main idea of Swin Transformer is to introduce several important visual priors into the vanilla Transformer encoder, including hierarchy, locality, and translation invariance, which combines the strength of both: the basic Transformer unit has strong modeling capabilities, and the visual priors make it friendly to a variety of visual tasks. \n\n\\paragraph{Normalization configuration} It is widely known that normalization technologies~\\cite{ioffe2015batch, ba2016layer, wu2018group, ulyanov2017instance} are crucial in stably training deeper architectures. The original Swin Transformer inherits the common practice in the language Transformers~\\cite{radford2019language} and vanilla ViT~\\cite{dosovitskiy2020vit} to utilize a pre-normalization configuration without extensive study, as shown in the figure~\\ref{fig:teaser}. In the following subsections, we will examine this default normalization configuration\\footnote{There have been a few alternative normalization configurations, such as post-normalization~\\cite{vaswani2017attention} and sandwich normalization~\\cite{ding2021cogview}. Post-normalization harms training stability~\\cite{xiongLN2020}, and sandwich normalization sacrifices representation power due to too many normalization layers.}. \n\n\\paragraph{Relative position bias} is a key component in the original Swin Transformer which introduces an additional parametric bias term to encode the geometric relationship in self-attention calculation:\n\\begin{equation}\n\\label{eq.att}\n    \\text{Attention}(Q, K, V) = \\text{SoftMax}(QK^T/\\sqrt{d}+B)V,\n\\end{equation}\nwhere $B \\in \\mathbb{R}^{M^2 \\times M^2}$ is the relative position bias term for each head; $Q, K, V \\in \\mathbb{R}^{M^2\\times d}$ are the \\emph{query}, \\emph{key} and \\emph{value} matrices; $d$ is the \\emph{query}/\\emph{key} dimension, and $M^2$ is the number of patches in a window. The relative position bias encodes relative spatial configurations of visual elements and is shown critical in a variety of visual tasks, especially for dense recognition tasks such as object detection.\n\nIn Swin Transformer, the relative positions along each axis are within the range of $[-M+1, M-1]$ and the relative position bias is parameterized as a bias matrix $\\hat{B} \\in \\mathbb{R}^{(2M-1)\\times (2M-1)}$, and the elements in $B$ are taken from $\\hat{B}$. When transferring across different window sizes, the learnt relative position bias matrix in pre-training is used to initialize the bias matrix of a different size in fine-tuning by bi-cubic interpolation.\n\n\\paragraph{Issues in scaling up model capacity and window resolution} We observe two issues when we scale up the capacity and window resolution of the Swin Transformer.\n\n\n\\begin{table*}[t]\n\\centering\n\\small\n\\addtolength{\\tabcolsep}{-3.1pt}\n\\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c}\n\\Xhline{1.0pt}\n& ImageNet* & \\multicolumn{4}{c|}{ImageNet$^\\dag$} & \\multicolumn{2}{c|}{COCO} & \\multicolumn{2}{c}{ADE20k} \\\\\n\\cline{2-11}\nmethod & \\makecell{W8, I256 \\\\ top-1 acc} & \\makecell{W12, I384 \\\\ top-1 acc} & \\makecell{W16, I512\\\\top-1 acc} & \\makecell{W20, I640 \\\\top-1 acc} & \\makecell{W24, I768\\\\top-1 acc} & \\makecell{W16\\\\AP$^\\text{box}$} & \\makecell{W32\\\\AP$^\\text{box}$} & \\makecell{W16\\\\mIoU}& \\makecell{W20\\\\mIoU} &\\makecell{W32\\\\mIoU} \\\\\n\\hline\nParameterized position bias~\\cite{liu2021swin} & 81.7 & 79.4/82.7 & 77.2/83.0 & 73.2/83.2 & 68.7/83.2 & 50.8 & 50.9 &45.5 & 45.8 & 44.5\\\\\n\\hline\nLinear-Spaced CPB & \\makecell{81.7\\\\\\shl{(+0.0)}} & \\makecell{82.0/82.9\\\\\\shl{(+2.6/+0.2)}} & \\makecell{81.2/83.3\\\\\\shl{(+4.0/+0.3)}} & \\makecell{79.8/83.6\\\\\\shl{(+6.6/+0.4)}} & \\makecell{77.6/83.7\\\\\\shl{(+8.9/+0.5)}} & \\makecell{50.9\\\\\\shl{(+0.1)}} &\\makecell{51.7\\\\\\shl{(+0.8)}}  & \\makecell{47.0\\\\\\shl{(+1.5)}}&\\makecell{47.4\\\\\\shl{(+1.6)}} & \\makecell{47.2\\\\\\shl{(+2.7)}}\\\\\n\\hline\nLog-Spaced CPB & \\makecell{81.8\\\\\\hl{(+0.1)}} &\\makecell{82.4/83.2\\\\\\hl{(+3.0/+0.5)}} &\\makecell{81.7/83.8\\\\\\hl{(+4.5/+0.8)}} & \\makecell{80.4/84.0\\\\\\hl{(+7.2/+0.8)}} & \\makecell{79.1/84.2\\\\\\hl{(+10.4/+1.0)}} &\\makecell{51.1\\\\\\hl{(+0.3)}} & \\makecell{51.8\\\\\\hl{(+0.9)}} & \\makecell{47.0\\\\\\hl{(+1.5)}} &\\makecell{47.7\\\\\\hl{(+1.9)}} & \\makecell{47.8\\\\\\hl{(+3.3)}} \\\\\n\\Xhline{1.0pt}\n\\end{tabular}\n\\caption{Comparison of different position bias computation approaches using Swin-T. * indicates the top-1 accuracy on ImageNet-1k trained from scratch. The models in * column will be used for testing on the ImageNet-1K image classification task using larger image/window resolutions, marked by $\\dag$. For these results, we report both the results w.o./with fine-tuning. These models are also used for fine-tuning on COCO object detection and ADE20K semantic segmentation tasks.}\n\\label{tab:lcpb}\n\\vspace{-1em}\n\\end{table*}\n\n\\begin{itemize}\n\\item \\emph{An instability issue when scaling up model capacity}. As shown in Figure~\\ref{fig:act}, when we scale up the original Swin Transformer model from small size to large size, the activation values at deeper layers increase dramatically. The discrepancy between layers with the highest and the lowest amplitudes has reached an extreme value of $10^4$. When we scale it up further to a huge size (658 million parameters), it cannot complete the training, as shown in Figure~\\ref{fig:divergence_of_huge_v1}.\n\n\\item \\emph{Degraded performance when transferring models across window resolutions}. As shown in the first row of Table~\\ref{tab:lcpb}, the accuracy decreases significantly when we directly test the accuracy of a pre-trained ImageNet-1K model ($256\\times 256$ images with $8\\times 8$ window size) at larger image resolutions and window sizes through the bi-cubic interpolation approach. It may be worth re-examining the relative position bias approach in the original Swin Transformer.\n\n\\end{itemize}\n\n\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.95\\linewidth]{figs/act5.pdf}\n    \\vspace{-1em}\n    \\caption{The Signal Propagation Plot~\\cite{yao2021leveraging,brock2021characterizing} for various model sizes. H-size models are trained at a self-supervised learning phase, and other sizes are trained by an image classification task. * indicates that we use a 40-epoch model before it crashes.}\n    \\label{fig:act}\n    \\vspace{-1em}\n\\end{figure}\n\n\nIn the following subsections, we present techniques to address these issues, including \\emph{residual post normalization} and \\emph{scaled cosine attention} to address the instability issue, and a \\emph{log-spaced continuous position bias} approach to address the issue in transferring across window resolutions.\n\n\\subsection{Scaling Up Model Capacity}\n\nAs mentioned in Section~\\ref{sec.swin_v1}, the original Swin Transformer (and most vision Transformers) adopts a layer norm layer at the beginning of each block, inherited from vanilla ViT. When we scale up the model capacity, a significant increase in activation values is observed at deeper layers. In fact, in a pre-normalization configuration, the output activation values of each residual block are merged directly back to the main branch, and the amplitude of the main branch grows larger and larger at deeper layers. Large amplitude discrepancy in different layers causes training instability.\n\n\\paragraph{Post normalization}\nTo ease this problem, we propose to use a \\emph{residual post normalization} approach instead, as shown in Figure~\\ref{fig:teaser}. In this approach, the output of each residual block is normalized before merging back into the main branch, and the amplitude of the main branch does not accumulate when the layer goes deeper. As shown in Figure~\\ref{fig:act}, the activation amplitudes by this approach are much milder than in the original pre-normalization configuration. \n\nIn our largest model training, we introduce an additional layer normalization layer on the main branch every 6 Transformer blocks, to further stabilize training.\n\n\\paragraph{Scaled cosine attention} In the original self-attention computation, the similarity terms of the pixel pairs are computed as a dot product of the \\emph{query} and \\emph{key} vectors. We find that when this approach is used in large visual models, the learnt attention maps of some blocks and heads are frequently dominated by a few pixel pairs, especially in the \\emph{res-post-norm} configuration. To ease this issue, we propose a \\emph{scaled cosine attention} approach that computes the attention logit of a pixel pair $i$ and $j$ by a scaled cosine function:\n\\begin{equation}\n\\label{eq.att}\n    \\text{Sim}(\\mathbf{q}_i, \\mathbf{k}_j) = \\text{cos}(\\mathbf{q}_i, \\mathbf{k}_j) / \\tau + B_{ij},\n\\end{equation}\nwhere $B_{ij}$ is the relative position bias between pixel $i$ and $j$; $\\tau$ is a learnable scalar, non-shared across heads and layers. $\\tau$ is set larger than 0.01. The cosine function is naturally normalized, and thus can have milder attention values.\n\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{figs/h_loss2.pdf}\n    \\caption{SwinV1-H versus SwinV2-H in training~\\cite{simmim}.}\n    \\label{fig:divergence_of_huge_v1}\n    \\vspace{-1em}\n\\end{figure}\n\n\\subsection{Scaling Up Window Resolution}\n\nIn this subsection, we introduce a log-spaced continuous position bias approach, so that the relative position bias can be smoothly transferred across window resolutions.\n\n\\paragraph{Continuous relative position bias} Instead of directly optimizing the parameterized biases, the \\emph{continuous} position bias approach adopts a small meta network on the relative coordinates:\n\\begin{equation}\n\\label{eq.cpb}\n    B (\\Delta x, \\Delta y) = \\mathcal{G} (\\Delta x, \\Delta y),\n\\end{equation}\nwhere $\\mathcal{G}$ is a small network, e.g., a 2-layer MLP with a ReLU activation in between by default.\n\nThe meta network $\\mathcal{G}$ generates bias values for arbitrary relative coordinates, and thus can be naturally transferred to fine-tuning tasks with arbitrarily varying window sizes. In inference, the bias values at each relative position can be pre-computed and stored as model parameters, such that the inference is the same as the original parameterized bias approach.\n\n\\paragraph{Log-spaced coordinates} \n\nWhen transferring across largely varying window sizes, a large portion of the relative coordinate range needs to be extrapolated. To ease this issue, we propose using log-spaced coordinates instead of the original linear-spaced ones:\n\\begin{equation}\n\\begin{aligned}\n\\label{eq.log_coord}\n    \\widehat{\\Delta x} = \\text{sign}(x) \\cdot \\log(1+|\\Delta x|), \\\\\n    \\widehat{\\Delta y} = \\text{sign}(y) \\cdot \\log(1+|\\Delta y|),\n\\end{aligned}\n\\end{equation}\nwhere $\\Delta x$, $\\Delta y$ and $\\widehat{\\Delta x}$, $\\widehat{\\Delta y}$ are the linear-scaled and log-spaced coordinates, respectively.\n\nBy using the log-spaced coordinates, when we transfer the relative position biases across window resolutions, the required extrapolation ratio will be much smaller than that of using the original linear-spaced coordinates. For an example of transferring from a pre-trained $8\\times 8$ window size to a fine-tuned $16\\times 16$ window size, using the original raw coordinates, the input coordinate range will be from $[-7, 7]\\times [-7, 7]$ to $[-15, 15]\\times [-15, 15]$. The extrapolation ratio is $\\frac{8}{7}=1.14\\times$ of the original range. Using log-spaced coordinates, the input range will be from $[-2.079, 2.079]\\times [-2.079, 2.079]$ to $[-2.773, 2.773]\\times [-2.773, 2.773]$. The extrapolation ratio is $0.33\\times$ of the original range, which is an about 4 times smaller extrapolation ratio than that using the original linear-spaced coordinates.\n\nTable~\\ref{tab:lcpb} compares the transferring performance of different position bias computation approaches. It can be seen that the log-spaced CPB (continuous position bias) approach performs best, particularly when transferred to larger window sizes.\n\n\\subsection{Self-Supervised Pre-training}\nLarger models are more data hungry. To address the data hungry problem, previous large vision models typically utilize huge labelled data such as JFT-3B~\\cite{zhai2021scaling,riquelme2021scaling,dai2021coatnet}. In this work, we exploit a self-supervised pre-training method, SimMIM~\\cite{simmim}, to alleviate the demands on labelled data. By this approach, we successfully trained a powerful Swin Transformer model of 3 billion parameters which achieves state-of-the-art (SOTA) on 4 representative visual benchmarks, by using only 70 million labelled images (1/40 of that in JFT-3B).\n\n\\subsection{Implementation to Save GPU Memory}\n\nAnother issue lies in the unaffordable GPU memory consumption with a regular implementation when both the capacity and resolution are large. To facility the memory issue, we adopt the following implementations:\n\\begin{itemize}\n    \\item \\emph{Zero-Redundancy Optimizer (ZeRO)}~\\cite{rajbhandari2020zero}. In a general data-parallel implementation of optimizers, the model parameters and optimization states are broadcasted to every GPU. This implementation is very unfriendly on GPU memory consumption, for example, a model of 3 billion parameters will consume 48G GPU memory when an AdamW optimizer and fp32 weights/states are used. With a ZeRO optimizer, the model parameters and the corresponding optimization states will be split and distributed to multiple GPUs, which significantly reduces memory consumption. We adopt the DeepSpeed framework and use the ZeRO stage-1 option in our experiments. This optimization has little effect on training speed.\n    \\item \\emph{Activation check-pointing}~\\cite{chen2016training}. Feature maps in the Transformer layers also consume a lot of GPU memory, which can create bottlenecks when image and window resolutions are high. The activation check-pointing technology can significantly reduce the memory consumption, while the training speed is up to 30\\% slower.\n    \\item \\emph{Sequential self-attention computation}. To train large models on very large resolutions, for example, an image of 1,536$\\times$1,536 resolution with a window size of 32$\\times$32, regular A100 GPUs (40GB memory) are still unaffordable, even with the above two optimization technologies. We found that in this case, the self-attention module constitutes a bottleneck. To alleviate this problem, we implement self-attention computation sequentially, instead of using the previous batch computation approach. This optimization is applied to the layers in the first two stages and has little impact on the overall training speed.\n\\end{itemize}\n\nWith these implementations, we managed to train a 3B model using the Nvidia A100-40G GPUs for COCO object detection with an input image resolution of 1,536$\\times$1,536, and Kinetics-400 action classification with an input resolution of $320\\times 320 \\times 8$.\n\n\n\n\\subsection{Model configurations}\n\nWe maintain the stage, block, and channel settings of the original Swin Transformer for 4 configurations of Swin Transformer V2:\n\\begin{itemize}\n    \\item SwinV2-T: $C$ = $96$, \\#. block = $\\{2, 2, 6, 2\\}$\n    \\item SwinV2-S/B/L: $C$=$96/128/192$, \\#.block=$\\{2, 2, 18, 2\\}$\n\\end{itemize}\nwith $C$ the number of channels in the first stage. \n\nWe further scale up Swin Transformer V2 to its huge size and giant size, with 658 million parameters and 3 billion parameters, respectively:\n\\begin{itemize}\n    \\item SwinV2-H: $C=352$, \\#. block = $\\{2, 2, 18, 2\\}$\n    \\item SwinV2-G: $C=512$, \\#. block = $\\{2, 2, 42, 4\\}$\n\\end{itemize}\nFor SwinV2-H and SwinV2-G, we add an additional layer normalization layer on the main branch every 6 layers. To save experimental time, we only employ SwinV2-G for large-scale experiments. %on various vision tasks. \nSwinV2-H is employed for another parallel study about self-supervised learning~\\cite{simmim}.\n\n\\section{Experiments}\n\n\\subsection{Tasks and Datasets}\n\n\nWe conduct experiments on ImageNet-1K image classification (V1 and V2)~\\cite{deng2009imagenet,recht2019imagenet}, COCO object detection~\\cite{lin2014coco}, and ADE20K semantic segmentation~\\cite{zhou2018semantic}. For the 3B model experiments, we also report the accuracy on Kinetics-400 video action recognition~\\cite{kay2017kinetics}.\n\n\\begin{itemize}\n    \\item \\emph{Image classification}. ImageNet-1K V1 and V2 val are used~\\cite{deng2009imagenet,recht2019imagenet} for evaluation. ImageNet-22K~\\cite{deng2009imagenet} which has 14M images and 22K categories is optionally employed for pre-training.  For the pre-training our largest model SwinV2-G, a privately collected ImageNet-22K-ext dataset with 70 million images is used. For this dataset, a duplicate removal process~\\cite{radford2021clip} is conducted to exclude overlapping images with ImageNet-1K V1 and V2 validation sets.\n    \\item \\emph{Object detection}. COCO~\\cite{lin2014coco} is used for evaluation. For our largest model experiments, we employ an additional detection pre-training phase using Object 365 v2 dataset~\\cite{Shao_2019_ICCV}, in-between the image classification pre-training phase and the COCO fine-tuning phase.\n    \\item \\emph{Semantic segmentation}. ADE20K~\\cite{zhou2018semantic} is used.\n    \\item \\emph{Video action classification}. Kinetics-400 (K400)~\\cite{kay2017kinetics} is used in evaluation. \n\\end{itemize}\n\nThe pre-training and fine-tuning settings will be detailed in Appendix.\n\n\n\\begin{table*}[t]\n    \\centering\n    \\small\n    \\addtolength{\\tabcolsep}{-2.0pt}\n    \\begin{tabular}{ccccccccc}\n    \\Xhline{1.0pt}\n    Method & param & \\makecell{ pre-train\\\\images} & \\makecell{ pre-train\\\\length (\\#im)} & \\makecell{pre-train\\\\im size} & \\makecell{pre-train\\\\time} & \\makecell{fine-tune\\\\im size} & \\makecell{ImageNet-1K-V1\\\\top-1 acc} & \\makecell{ImaegNet-1K-V2\\\\top-1 acc}\\\\\n    \\hline\n    SwinV1-B & 88M & IN-22K-14M & 1.3B & 224$^2$ & $<$30$^\\dag$ & 384$^2$ &86.4 &  76.58\\\\\n    SwinV1-L & 197M & IN-22K-14M & 1.3B & 224$^2$ & $<$10$^\\dag$ & 384$^2$ &  87.3 & 77.46\\\\\n    \\hline\n    ViT-G~\\cite{zhai2021scaling} & 1.8B & JFT-3B & 164B & 224$^2$ & $>$30k & 518$^2$ & 90.45 & 83.33 \\\\\n    V-MoE~\\cite{riquelme2021scaling} & 14.7B* & JFT-3B & - & 224$^2$ & 16.8k & 518$^2$ & 90.35 & - \\\\\n    CoAtNet-7~\\cite{dai2021coatnet} & 2.44B & JFT-3B & - & 224$^2$ & 20.1k & 512$^2$ & \\textbf{90.88} & - \\\\\n    \\hline\n    SwinV2-B & 88M & IN-22K-14M & 1.3B & 192$^2$ & $<$30$^\\dag$ & 384$^2$ & 87.1 & 78.08\\\\\n    SwinV2-L & 197M & IN-22K-14M & 1.3B & 192$^2$ & $<$20$^\\dag$ & 384$^2$ & 87.7 & 78.31\\\\\n    { SwinV2-G} & 3.0B & IN-22K-ext-70M & 3.5B & 192$^2$ & $<$0.5k$^\\dag$ & 640$^2$  & 90.17 & \\textbf{84.00} \\\\\n    \\Xhline{1.0pt}\n    \\end{tabular}\n    \\caption{Comparison with previous largest vision models on ImageNet-1K V1 and V2 classification. * indicates the sparse model; the ``pre-train time'' column is measured by the TPUv3 core days with numbers copied from the original papers. $\\dag$ That of SwinV2-G is estimated according to training iterations and FLOPs.}\n    \\label{tab:sota_imagenet}\n    \\vspace{-1em}\n\\end{table*}\n\n\n\\begin{table}[t]\n    \\centering\n    \\small\n    \\addtolength{\\tabcolsep}{-4.0pt}\n    \\begin{tabular}{c|cc|cc|cc}\n    \\Xhline{1.0pt}\n     \\multirow{2}{*}{Method} & \\multirow{2}{*}{\\makecell{train\\\\I(W) size}} & \\multirow{2}{*}{\\makecell{test\\\\I(W) size}} &\\multicolumn{2}{c|}{{\\scriptsize mini-val (AP)}} & \\multicolumn{2}{c}{{\\scriptsize test-dev (AP)}}\\\\\n    & & & box &  mask & box &  mask\\\\\n    \\hline\n    CopyPaste\\cite{ghiasi2020copy} & 1280(-) & 1280(-) & 57.0 & 48.9 & 57.3 & 49.1 \\\\\n    SwinV1-L\\cite{liu2021swin} & 800(7)& ms(7)& 58.0 & 50.4 & 58.7 & 51.1\\\\\n    YOLOR\\cite{wang2021learn} & 1280(-) & 1280(-) & -&- & 57.3 & -\\\\\n    CBNet\\cite{liang2021cbnetv2} & 1400(7) & ms(7) & 59.6& 51.8& 60.1 & 52.3\\\\\n    DyHead\\cite{dai2021dynamic} & 1200(-) & ms(-)& 60.3 & - & 60.6 & - \\\\\n    {\\scriptsize SoftTeacher}\\cite{xu2021endtoend} & 1280(12) & ms(12) & 60.7 & 52.5 & 61.3 & 53.0 \\\\\n    \\hline\n    \\multirow{3}{*}{\\makecell{SwinV2-L\\\\(HTC++)}} & \\multirow{3}{*}{1536(32)}& 1100(32) & 58.8 & 51.1& - & -\\\\\n     & & 1100 (48) & 58.9 &  51.2& - & -\\\\\n     & & ms (48) & 60.2 & 52.1 &60.8 &52.7\\\\\n    \\hline\n    \\multirow{3}{*}{\\makecell{SwinV2-G\\\\(HTC++)}} & \\multirow{3}{*}{1536(32)}& 1100(32) & 61.7 & 53.3 & - & -\\\\\n     & & 1100 (48) & 61.9 & 53.4 & - & -\\\\\n    & & ms (48) & \\textbf{62.5} & \\textbf{53.7} & \\textbf{63.1} & \\textbf{54.4}\\\\\n    \\Xhline{1.0pt}\n    \\end{tabular}\n    \\caption{Comparison with previous best results on COCO object detection and instance segmentation. I(W) indicates the image and window size. ms indicate multi-scale testing is employed.}\n    \\label{tab:sota_coco}\n    \\vspace{-1em}\n\\end{table}\n\n\n\\begin{table}[t]\n    \\centering\n    \\small\n    \\addtolength{\\tabcolsep}{-4.0pt}\n    \\begin{tabular}{c|cc|c}\n    \\Xhline{1.0pt}\n     Method & {\\makecell{train I(W) size}} & {\\makecell{test I(W) size}} & mIoU \\\\\n    \\hline\n    SwinV1-L\\cite{liu2021swin} & 640(7) & 640(7) & 53.5*\\\\\n    Focal-L\\cite{yang2021focal} & 640(40) & 640(40) & 55.4*\\\\\n    CSwin-L\\cite{dong2021cswin} & 640(40) & 640(40) & 55.7*\\\\\n    MaskFormer\\cite{cheng2021maskformer} & 640(7) & 640(7) & 55.6*\\\\\n    FaPN\\cite{huang2021fapn} & 640(7) & 640(7) & 56.7* \\\\\n    BEiT\\cite{bao2021beit} & 640(40) & 640(40) & 58.4* \\\\\n    \\hline\n    \\makecell{SwinV2-L\\\\(UperNet)} & 640(40) & 640(40) & 55.9* \\\\\n    \\hline\n    \\multirow{3}{*}{\\makecell{SwinV2-G\\\\(UperNet)}} & \\multirow{3}{*}{640(40)}& 640(40) & 59.1 \\\\\n     & & 896 (56) & 59.3 \\\\\n    & & 896 (56) & \\textbf{59.9}*\\\\\n    \\Xhline{1.0pt}\n    \\end{tabular}\n    \\caption{Comparison with previous best results on ADE20K semantic segmentation. * indicates multi-scale testing is used.}\n    \\label{tab:sota_ade}\n    \\vspace{-1em}\n\\end{table}\n\n\n\\begin{table}[htb]\n    \\centering\n    \\small\n    \\addtolength{\\tabcolsep}{-4.0pt}\n    \\begin{tabular}{c|cccc}\n    \\Xhline{1.0pt}\n     Method & {\\makecell{train I(W) size}} & {\\makecell{test I(W) size}} & views & top-1\\\\\n    \\hline\n    ViViT\\cite{arnab2021vivit} & \\makecell{-(-)} & -(-) & 4$\\times$3 & 84.8 \\\\\n    SwinV1-L\\cite{liu2021video} & \\makecell{480(12)$^2\\times$16(8)} & \\makecell{480(12)$^2\\times$16(8)} & 10$\\times$5 & 84.9 \\\\\n    TokenLearner\\cite{ryoo2021tokenlearner} & \\makecell{256(8)$^2\\times$64(64)} & \\makecell{256(8)$^2\\times$64(64)} & 4$\\times$3 & 85.4\\\\\n    \\hline\n    \\multirow{3}{*}{\\makecell{Video-SwinV2-G}} & \\multirow{3}{*}{\\makecell{320(20)$^2\\times$8(8)}} & \\makecell{320(20)$^2\\times$8(8)} & 1$\\times$1 & 83.2\\\\\n     & & \\makecell{384(24)$^2\\times$8(8)} & 1$\\times$1 & 83.4 \\\\\n     & & \\makecell{384(24)$^2\\times$8(8)} & 4$\\times$5 & \\textbf{86.8} \\\\\n    \\Xhline{1.0pt}\n    \\end{tabular}\n    \\caption{Comparison with previous best results on Kinetics-400 video action classification.}\n    \\label{tab:sota_kinetics}\n    \\vspace{-1em}\n\\end{table}\n\n\\subsection{Scaling Up Experiments}\n\nWe first present the results on various representative visual benchmarks by scaling up models to 3 billion parameters and to high image/window resolutions.\n\n\\paragraph{Settings for SwinV2-G experiments}\n\nWe adopt a smaller $192\\times 192$ image resolution in pre-training to save on training costs. We take a 2-step pre-training approach. First, the model is pre-trained using a self-supervised method~\\cite{simmim} on the ImageNet-22K-ext dataset by 20 epochs. Second, the model is further pre-trained by 30 epochs using the image classification task on this dataset. Detailed pre-training and fine-tuning setups are described in the appendix.\n\nIn the following paragraphs, we report the accuracy of SwinV2-G on representative vision benchmarks. Note that since our main goal is to explore how to feasibly scale up model capacity and window resolution, and whether the vision tasks can benefit from significantly larger capacity, we did not particularly align complexities or pre-training data in comparisons.\n\n\\paragraph{ImageNet-1K image classification results}\n\nTable~\\ref{tab:sota_imagenet} compares the SwinV2-G model with previously largest/best vision models on ImageNet-1K V1 and V2 classification. SwinV2-G is the largest dense vision model to present. It achieves a top-1 accuracy of 84.0\\% on the ImageNet V2 benchmark, which is +0.7\\% higher than previous best one (83.3\\%). Our accuracy on ImageNet-1K V1 is marginally lower (90.17\\% vs 90.88\\%). The performance difference might come from different degrees of dataset over-tuning~\\cite{recht2019imagenet}. Also note we employ much less training iterations and lower image resolutions than those in previous efforts, while performing very well.\n\nWe also compare the SwinV2-B and SwinV2-L to the original SwinV1-B and SwinV1-L, respectively, where a +0.8\\% and +0.4\\% gains are observed. The shrunken gains by SwinV2-L than that of SwinV2-B may imply that if exceeding this size, more labeled data, stronger regularization, or advanced self-supervised learning methods are required.\n\n\\vspace{-0.5em}\n\\paragraph{COCO object detection results}\n\nTable~\\ref{tab:sota_coco} compares the SwinV2-G model with previous best results on COCO object detection and instance segmentation. It achieves 63.1/54.4 box/max AP on COCO test-dev, which is +1.8/1.4 higher than previous best numberw (61.3/53.0 by \\cite{xu2021endtoend}). This suggests that scaling up vision model is beneficial for the dense vision recognition task of object detection. Our approach can use a different window size at test to additionally benefit, probably attributed to the effective Log-spaced CPB approach.\n\n\\vspace{-0.5em}\n\n\\paragraph{ADE20K semantic segmentation results}\n\nTable~\\ref{tab:sota_ade} compares the SwinV2-G model with previous best results on the ADE20K semantic segmentation benchmark. It achieves 59.9 mIoU on ADE20K val set, +1.5 higher than the previous best number (58.4 by \\cite{bao2021beit}). This suggests scaling up vision model is beneficial for pixel-level vision recognition tasks. Using a larger window size at test time can additionally bring +0.2 gains, probably attributed to the effective Log-spaced CPB approach.\n\n\\vspace{-0.5em}\n\n\\paragraph{Kinetics-400 video action classification results}\n\nTable~\\ref{tab:sota_kinetics} compares the SwinV2-G model with previous best results on the Kinetics-400 action classification benchmark. It achieves 86.8\\% top-1 accuracy, +1.4\\% higher than previous best number~\\cite{ryoo2021tokenlearner}. This suggests that scaling up vision models also benefits video recognition tasks. In this scenario, using a larger window size at test time can also bring additional benefits of +0.2\\%, probably attributed to the effective Log-spaced CPB approach.\n\n\n\\begin{table}[t]\n    \\centering\n    \\small\n    \\begin{tabular}{c|c|c|c}\n    \\Xhline{1.0pt}\n    Backbone & res-post-norm & \\makecell{scaled cosine\\\\attention} & \\makecell{ImageNet \\\\ top-1 acc} \\\\\n    \\hline\n    \\multirow{3}{*}{Swin-T} & & & 81.5 \\\\\n    & $\\checkmark$ & & 81.6\\\\\n    & $\\checkmark$ & $\\checkmark$ & \\textbf{81.7} \\\\\n    \\hline\n    \\multirow{3}{*}{Swin-S} & & &83.2 \\\\\n    & $\\checkmark$ & & 83.3\\\\\n    & $\\checkmark$ & $\\checkmark$ & \\textbf{83.6} \\\\\n    \\hline\n    \\multirow{3}{*}{Swin-B} & & &83.6 \\\\\n    & $\\checkmark$ & & 83.8\\\\\n    & $\\checkmark$ & $\\checkmark$ & \\textbf{84.1} \\\\  \n     \\hline\n     \\hline\n    \\multirow{2}{*}{ViT-B} & & &82.2 \\\\\n    & $\\checkmark$ & $\\checkmark$ & \\textbf{82.6} \\\\ \n    \\Xhline{1.0pt}\n    \\end{tabular}\n    \\caption{Ablation on res-post-norm and cosine attention.}\n    \\label{tab:postnorm_cosatt}\n    \\vspace{-1em}\n\\end{table}\n\n\\begin{table}[t]\n    \\centering\n    \\small\n    \\addtolength{\\tabcolsep}{-2.0pt}\n    \\begin{tabular}{c|c|c|c|c}\n    \\Xhline{1.0pt}\n    Backbone & pre-norm & \\makecell{sandwich}~\\cite{ding2021cogview}  & \\makecell{post-norm}~\\cite{vaswani2017attention} & \\makecell{our}  \\\\\n    \\hline\n    Swin-S & 83.2 & 82.6 & 83.3 & \\textbf{83.6} \\\\\n    Swin-B & 83.6 & - & 83.6 & \\textbf{84.1} \\\\\n    \\Xhline{1.0pt}\n    \\end{tabular}\n    \\caption{Comparison with other normalization methods. The post-norm method diverges at the default learning rate, and we use 1/4 of the default learning rate for this method. Sandwich performs worse than ours, probably because it sacrifices expressiveness.}\n    \\label{tab:norm}\n    \\vspace{-1em}\n\\end{table}\n\n\\subsection{Ablation Study}\n\n\\paragraph{Ablation on res-post-norm and scaled cosine attention} Table~\\ref{tab:postnorm_cosatt} ablates the performance of applying the proposed res-post-norm and scaled cosine attention approaches to Swin Transformer. Both techniques improve the accuracy at all the tiny, small and base size, and the overall improvements are +0.2\\%, +0.4\\% and +0.5\\% respectively, indicating the techniques are more beneficial for larger models. It also turns out to benefit ViT architecture (+0.4\\%). The proposed normalization approach also performs better than some other normalization methods, as shown in Table~\\ref{tab:norm}.\n\nMore importantly, the combination of post-norm and scaled cosine attention stabilize the training. As shown in Figure~\\ref{fig:act}, while the activation values at deeper layers for the original Swin Transformer are almost exploded at large (L) size, those of the new version have much milder behavior. On a huge size model, the self-supervised pre-training~\\cite{simmim} diverges using the original Swin Transformer, while it trains well by a Swin Transformer V2 model.\n\n\\vspace{-0.5em}\n\n\\paragraph{Scaling up window resolution by different approaches} Table~\\ref{tab:lcpb} and \\ref{tab:ablate_cpb_size} ablate the performance of 3 approaches by scaling window resolutions from $256\\times 256$ in pre-training to larger sizes in 3 down-stream vision tasks of ImageNet-1K image classification, COCO object detection, and ADE20K semantic segmentation, respectively. It can be seen that: 1) Different approaches have similar accuracy in pre-training (81.7\\%-81.8\\%); 2) When transferred to down-stream tasks, the two continuous position bias (CPB) approaches perform consistently better than the parameterized position bias approach used in Swin Transformer V1. Compared to the linear-spaced approach, the log-spaced version is marginally better; 3) The larger the change in resolutions between pre-training and fine-tuning, the larger the benefit of the proposed log-spaced CPB approach. %Table~\\ref{tab:ablate_cpb_size} ablates the effects of L-CPB using larger model sizes, suggesting the L-CPB approach.\n\nIn Table~\\ref{tab:lcpb} and \\ref{tab:ablate_cpb_size}, we also report the accuracy using targeted window resolutions without fine-tuning (see the first number in each column in the ImageNet-1K experiments). The recognition accuracy remains not bad even when the window size is enlarged from $8$ to $24$ (78.9\\% versus 81.8\\%), while the top-1 accuracy of the original approach significantly degrades from 81.7\\% to 68.7\\%. Also note that without fine-tuning, using a window size of $12$ that the pre-trained model has never seen before can even be +0.4\\% higher that the original accuracy. This suggests that we can improve accuracy through test-time window adjustment, as also observed in Table~\\ref{tab:sota_coco}, \\ref{tab:sota_ade} and \\ref{tab:sota_kinetics}.\n\n\n\\begin{table}[t]\n    \\centering\n    \\small\n\\addtolength{\\tabcolsep}{-2.5pt}\n    \\begin{tabular}{c|c|c|c|c}\n    \\Xhline{1.0pt}\n    & & ImageNet* & \\multicolumn{2}{c}{ImageNet$^\\dag$} \\\\\n    \\cline{3-5}\n    Backbone & L-CPB & \\makecell{W8, I256} & \\makecell{W12, I384 } & \\makecell{W16, I512}\\\\\n    \\hline\n    \\multirow{2}{*}{SwinV2-S} &   & 83.7 & 81.8/84.5 & 79.4/84.9\\\\\n    & $\\checkmark$ & 83.7 & 84.1/84.8 & 82.9/85.4 \\\\\n    \\hline\n    \\multirow{2}{*}{SwinV2-B} &  & 84.1 & 82.9/85.0 & 81.0/85.3\\\\\n    & $\\checkmark$ & 84.2 & 84.5/85.1 & 83.8/85.6 \\\\\n    \\Xhline{1.0pt}\n    \\end{tabular}\n    \\caption{Ablation on Log-CPB using different model sizes.}\n    \\label{tab:ablate_cpb_size}\n    \\vspace{-2em}\n\\end{table}\n\n\n\\section{Conclusion}\n\nWe have presented techniques for scaling Swin Transformer up to 3 billion parameters and making it capable of training with images of up to 1,536$\\times$1,536 resolution, including the \\emph{res-post-norm} and \\emph{scaled cosine attention} to make the model easier to be scaled up in capacity, as well a log-spaced continuous relative position bias approach which lets the model more effectively transferred across window resolutions. The adapted architecture is named Swin Transformer V2, and by scaling up capacity and resolution, it sets new records on 4 representative vision benchmarks. By these strong results, we hope to stimulate more research in this direction so that we can eventually close the capacity gap between vision and language models and facilitate the joint modeling of the two domains.\n\n\\section*{Acknowledgement}\n\nWe thank many colleagues at Microsoft for their help, in particular, Eric Chang, Lidong Zhou, Jing Tao, Aaron Zhang, Edward Cui, Bin Xiao, Lu Yuan, Peng Cheng, Fan Yang for useful discussion and the help on GPU resources and datasets.\n\n\n\\renewcommand{\\thesection}{A\\arabic{section}}\n\\setcounter{section}{0}\n\n\\section{Experimental Settings for Ablation}\n\nThis section describes the experimental settings for ablation, including models of SwinV2-T, SwinV2-S, and SwinV2-B, and tasks of ImageNet-1K image classification, COCO object detection and ADE semantic segmentation. \n\n\\subsection{ImageNet-1K Pre-training}\n\nAll ablation study use the ImageNet-1K image classification task for pre-training. We adopt an input image size (window size) of 256$\\times$256 (8$\\times$8)\\footnote{Most of our experiments have the window size as an even number to make the window shifting offset divisible by the window size. Nevertheless, an odd number of window size also works well, as is right the case in the original Swin Transformer ($7\\times 7$).}. Following \\cite{liu2021swin}, we employ an AdamW~\\cite{loshchilov2017decoupled} optimizer for 300 epochs using a cosine decay learning rate scheduler with 20 epochs of linear warm-up. A batch size of 1024, an initial learning rate of $1\\times10^{-3}$, a weight decay of 0.05, and gradient clipping with a max norm of 5.0 are used. Augmentation and regularization strategies include RandAugment~\\cite{cubuk2020randaugment}, Mixup~\\cite{zhang2017mixup}, Cutmix~\\cite{yun2019cutmix}, random erasing~\\cite{zhong2020random} and stochastic depth~\\cite{huang2016deep}. An increasing degree of stochastic depth augmentation is employed for larger models, i.e. $0.2, 0.3, 0.5$ for tiny, small, and base models, respectively. \n\n\\subsection{Fine-tuning on various tasks}\n\n\\paragraph{ImageNet-1K image classification} For ImageNet-1K image classification experiments, we conduct a fine-tuning step if the input image resolution is larger than that in the pre-training step. The fine-tuning lasts for 30 epochs, with an AdamW~\\cite{loshchilov2017decoupled} optimizer, a cosine decay learning rate scheduler with an initial learning rate of $4\\times10^{-5}$, a weight decay of $1\\times10^{-8}$, and the same data augmentation and regularizations as those in the first stage.\n\n\\paragraph{COCO object detection}\nWe use cascade mask R-CNN~\\cite{he2017mask,cai2018cascade} implemented in mmdetection~\\cite{chen2019mmdetection} as the object detection framework. In training, a multi-scale augmentation~\\cite{carion2020detr,sun2020sparsercnn} with the shorter side between 480 and 800 and the longer side of 1333 is used. The window size is set 16$\\times$16. An AdamW~\\cite{loshchilov2017decoupled} optimizer with an initial learning rate of $1\\times10^{-4}$, a weight decay of 0.05, a batch size of 16, and a 3$\\times$ scheduler are used.\n\n\\paragraph{ADE20K semantic segmentation}\nWe adopt an image size (window size) of 512$\\times$512 (16$\\times$16). In training, we employ an AdamW~\\cite{loshchilov2017decoupled} optimizer with an initial learning rate of $4\\times10^{-5}$, a weight decay of 0.05, a learning rate scheduler that uses linear learning rate decay and a linear warm-up of 1,500 iterations. Models are trained with batch size of 16 for 160K iterations. We follow the mmsegmentation codebase to adopt augmentations of random horizontal flipping, random re-scaling within ratio range [0.5, 2.0] and a random photometric distortion. Stochastic depth with ratio of $0.3$ is applied for all models. A layer-wise learning rate decay~\\cite{bao2021beit} of 0.95 is adopted for all experiments.\n\n\\section{Experimental Settings for System-Level Comparison}\n\n\\subsection{SwinV2-B and SwinV2-L Settings}\n\nTable 2, 3 and 4 include results of SwinV2-B and SwinV2-L. For these experiments, we first conduct ImageNet-22K pre-training, and then fine-tune the pre-trained models on individual down-stream recognition tasks.\n\n\\paragraph{ImageNet-22K pre-training} \n\nBoth models use an input image size (window size) of 192$\\times$192 (12$\\times$12). \nWe employ an AdamW optimizer~\\cite{loshchilov2017decoupled} for 90 epochs using a cosine learning rate scheduler with 5-epoch linear warm-up. A batch size of 4096, an initial learning rate of 0.001, a weight decay of 0.1, and gradient clipping with a max norm of 5.0 are used. Augmentation and regularization strategies include RandAugment~\\cite{cubuk2020randaugment}, Mixup~\\cite{zhang2017mixup}, Cutmix~\\cite{yun2019cutmix}, random erasing~\\cite{zhong2020random} and stochastic depth~\\cite{huang2016deep} with ratio of 0.2.\n\n\\paragraph{ImageNet-1K image classification} We consider input image sizes of 256$\\times$256 and 384$\\times$384. The training length is set 30 epochs, with a batch size of 1024, a cosine decay learning rate scheduler with an initial learning rate of $4\\times10^{-5}$, and a weight decay of $1\\times10^{-8}$. The ImageNet-1K classification weights are also initialized from the corresponding ones in the ImageNet-22K model.\n\n\n\\paragraph{COCO object detection}\nWe adopt HTC++~\\cite{chen2019htc,liu2021swin} for experiments. In data pre-processing, Instaboost~\\cite{fang2019instaboost}, a multi-scale training~\\cite{ghiasi2019fpn} with an input image size of 1536$\\times$1536, a window size of 32$\\times$32, and a random scale between $[0.1, 2.0]$ are used. An AdamW optimizer~\\cite{loshchilov2017decoupled} with an initial learning rate of $4\\times10^{-4}$ on batch size of 64, a weight decay of 0.05, and a $3\\times$ scheduler are used. The backbone learning rate is set $0.1\\times$ of the head learning rate. In inference, soft-NMS~\\cite{Bodla2017softnms} is used. Both single-scale and multi-scale test results are reported.\n\n\\paragraph{ADE20K semantic segmentation}\nThe input image size (window size) is set 640$\\times$640 (40$\\times$40). We employ an AdamW~\\cite{loshchilov2017decoupled} optimizer with an initial learning rate of $6\\times10^{-5}$, a weight decay of 0.05, a linear decayed learning rate scheduler with 375-iteration linear warm-up. The model is trained with batch size of 64 for 40K iterations. We follow the default settings in mmsegmentation for data augmentation, including random horizontal flipping, random re-scaling within ratio range $[0.5, 2.0]$ and random photometric distortion. Stochastic depth with ratio of $0.3$ is applied. \n\n\\subsection{SwinV2-G Settings}\n\n\\paragraph{Stage-1 self-supervised pre-training}\nThe model is first pre-trained using a self-supervised learning approach~\\cite{anonymous} on the ImageNet-22K-ext dataset (70 million images) for 20 epochs. To reduce experimental overheads, we adopt a smaller image size of 192$\\times$192. The model is trained using the AdamW~\\cite{loshchilov2017decoupled} optimizer with a cosine decay learning rate scheduler with 30000 steps of linear warm-up. A batch size of 9216, an initial learning rate of $1.4\\times10^{-3}$, a weight decay of 0.1, and gradient clipping with a max norm of 100.0 are used. A light data augmentation strategy is employed: random resize cropping with scale range of [0.67, 1] and a aspect ratio range of [3/4, 4/3], followed by a random flipping and a color normalization steps.\n\n\\paragraph{Stage-2 supervised pre-training}\nThe model is further pre-trained using the class labels on the ImageNet-22K-ext dataset. We employ an AdamW~\\cite{loshchilov2017decoupled} optimizer for 30 epochs, using a cosine decayed learning rate scheduler with 20000 steps of linear warm-up. A batch size of 9216, an initial learning rate of $1.4\\times10^{-3}$, a layer-wise learning rate decay of 0.87, a weight decay of 0.1, and gradient clipping with a max norm of 100.0 are used. Augmentation and regularization strategies include RandAugment~\\cite{cubuk2020randaugment}, random erasing~\\cite{zhong2020random} and a stochastic depth~\\cite{huang2016deep} ratio of 0.3.\n\n\\paragraph{Fine-tuning on ImageNet-1K image classification}\nWe adopt an input image size of 640$\\times$640 for experiments. An AdamW~\\cite{loshchilov2017decoupled} optimizer is employed for 10 epochs, using a cosine decayed learning rate scheduler and a 2-epoch linear warm-up. A batch size of 576, an initial learning rate of $2.1\\times10^{-5}$, a weight decay of 0.1, and gradient clipping with a max norm of 100.0 are used. Augmentation and regularization strategies include RandAugment~\\cite{cubuk2020randaugment}, random erasing~\\cite{zhong2020random} and a stochastic depth~\\cite{huang2016deep} ratio of 0.5.\n\nIn evaluation, we test top-1 accuracy on both ImageNet-1K V1 and V2.\n\n\\paragraph{Fine-tuning on COCO object detection}\nWe first conduct inter-mediate fine-tuning using the Objects-365 V2 dataset. In this stage, we remove the mask branch of the HTC++ framework~\\cite{chen2019htc,liu2021swin} because there are no mask annotations. The input image resolution and window size are set as $[800, 1024]$ and $32\\times 32$, respectively. In training, an  AdamW~\\cite{loshchilov2017decoupled} optimizer with initial learning rate of $1.2\\times10^{-3}$, a weight decay of 0.05 and a batch size of 96 are used, and the training length is set 67,500 steps.\n\nThen we fine-tune the HTC++ model on COCO dataset, with the mask branch randomly initialized and other model weights loaded from the Objects-365-V2 pre-trained model. In this training stage, the input image resolution is set 1536$\\times$1536 with a multi-scale ratio of $[0.1, 2.0]$. The window size is set 32$\\times$32. The  AdamW~\\cite{loshchilov2017decoupled} optimizer is employed, with an initial learning rate of $6\\times10^{-4}$, a weight decay of 0.05, and a batch size of 96, and is trained 45,000 steps.\n\nIn test, Soft-NMS~\\cite{Bodla2017softnms} is used. Both window sizes of $32\\times32$ and $48\\times 48$ are considered.\n \n\\paragraph{Fine-tuning on ADE20K semantic segmentation}\nThe input image size (window size) is set 640$\\times$640 (40$\\times$40). An AdamW optimizer~\\cite{loshchilov2017decoupled} is employed, with an initial learning rate of $4\\times10^{-5}$, a weight decay of 0.05, a linear decayed learning rate scheduler with 80K iterations, a batch size of 32, and a linear warm-up of 750 iterations. For augmentations, we follow the default settings in mmsegmentation to include random horizontal flipping, random re-scaling within ratio range $[0.5, 2.0]$ and random photometric distortion. The stochastic depth ratio is set $0.4$. \n\n\n\\paragraph{Fine-tuning on Kinetics-400 video action recognition}\nA 2-stage fine-tuning process is employed. In the first stage, an input resolution of 256$\\times$256$\\times$8 with 16$\\times$16$\\times$8 window size is adopted. We employ the AdamW optimizer for 20 epochs using a cosine decayed learning rate scheduler with 2.5-epoch linear warm-up. Other training hyper-parameters are: batch-size 80, an initial learning rate of $3.6\\times10^{-4}$, and a weight decay of 0.1.\n\nIn the second stage, we further fine-tune the model using a larger input video resolution of 320$\\times$320$\\times$8 with 20$\\times$20$\\times$8 window size. We employ the AdamW optimizer for 5 epochs using a cosine decayed learning rate scheduler with 1-epoch linear warm-up. A batch-size of 64, an initial learning rate of $5\\times10^{-5}$ and a weight decay of 0.1 are set.\n\n\\section{Learnt Relative Position Bias by Different Approaches}\n\nFigure~\\ref{fig:rpe_s0b0} visualizes the relative position bias matrices ($\\hat{B} \\in \\mathbb{R}^{(2M-1)\\times (2M-1)}$) learnt by different bias computation approaches, using a SwinV2-T model. The bias matrices of the 3 heads in the first block are visualized. The left shows the bias matrices learnt by using an input image size of 256$\\times$256 and a window size of $8\\times 8$. The right shows the bias matrices after fine-tuning on a larger input image resolution of 512$\\times$512 and a larger window size of 16$\\times$16. It turns out that the bias matrices learnt by two CPB(continuous position bias) approaches are more smoothly than that learnt by P-RPE (parameterized relative position bias). Figure~\\ref{fig:rpe_s3b0} shows more examples using the last block of this model.\n\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{figs/s0b0.pdf}\n    \\caption{Visualization of the learnt relative position bias matrices by different approaches, using a SwinV2-T model and the 3 heads in the first block. Left: the bias matrices by pre-training on a 256$\\times$256 image and a 8$\\times$8 window; Right: the bias matrices after fine-tuning using a 512$\\times$512 image size and 16$\\times$16 window size. H-x indicates the x-th head.}\n    \\label{fig:rpe_s0b0}\n\\end{figure*}\n\n\\begin{figure*}[ht]\n    \\centering\n    \\includegraphics[width=1.0\\linewidth]{figs/s3b0.pdf}\n    \\caption{Visualization of the learnt relative position bias matrices by different approaches, using a SwinV2-T model and the 24 heads in the last block. Left: the bias matrices by pre-training on a 256$\\times$256 image and a 8$\\times$8 window; Right: the bias matrices after fine-tuning using a 512$\\times$512 image size and 16$\\times$16 window size. H-x indicates the x-th head.}\n    \\label{fig:rpe_s3b0}\n\\end{figure*}\n\n%%%%%%%%% REFERENCES\n{\\small\n\\bibliographystyle{ieee_fullname}\n\\bibliography{egbib}\n}\n\n\\end{document}",
        "maxLength": 50000,
        "icon": "core/workflow/inputType/input"
      },
      {
        "key": "gKxpZiRI",
        "label": "code_knowledge",
        "type": "input",
        "description": "",
        "required": true,
        "valueType": "string",
        "defaultValue": "# Swin Transformer V2 实现代码分析\n\n## 1. 模块架构图\n\n```mermaid\ngraph TD\n    A[输入图像] --> B[PatchEmbed]\n    B --> C[Layer1]\n    C --> D[Layer2]\n    D --> E[Layer3]\n    E --> F[Layer4]\n    F --> G[分类头]\n    \n    subgraph \"PatchEmbed模块\"\n        B1[输入: B,C,H,W]\n        B2[卷积分块: kernel=4x4, stride=4]\n        B3[展平: B,Ph*Pw,C]\n        B4[输出: 嵌入特征]\n    end\n    \n    subgraph \"Transformer层结构\"\n        C1[BasicLayer]\n        C2[SwinTransformerBlock x N]\n        C3[WindowAttention]\n        C4[Mlp]\n        C5[PatchMerging]\n    end\n    \n    subgraph \"WindowAttention核心组件\"\n        W1[缩放余弦注意力]\n        W2[连续位置偏差MLP]\n        W3[对数间隔坐标变换]\n        W4[残差后归一化]\n    end\n    \n    A --> B1\n    B1 --> B2\n    B2 --> B3\n    B3 --> B4\n    B4 --> C1\n    C1 --> C2\n    C2 --> C3\n    C3 --> C4\n    C4 --> C5\n    C5 --> D\n```\n\n## 2. 数据流图\n\n```mermaid\nflowchart LR\n    subgraph \"输入数据\"\n        D1[(图像张量<br/>B×C×H×W)]\n        D2[(掩码张量<br/>B×H×W)]\n        D3[(配置参数)]\n    end\n    \n    subgraph \"特征提取管道\"\n        P1[PatchEmbed<br/>📥 输入: B×C×H×W<br/>🔄 处理: 4×4卷积分块<br/>📤 输出: B×L×C]\n        P2[BasicLayer1<br/>📥 输入: B×L×C<br/>🔄 处理: 2个Transformer块<br/>📤 输出: B×L/4×2C]\n        P3[BasicLayer2<br/>📥 输入: B×L/4×2C<br/>🔄 处理: 2个Transformer块<br/>📤 输出: B×L/16×4C]\n        P4[BasicLayer3<br/>📥 输入: B×L/16×4C<br/>🔄 处理: 18个Transformer块<br/>📤 输出: B×L/64×8C]\n        P5[BasicLayer4<br/>📥 输入: B×L/64×8C<br/>🔄 处理: 2个Transformer块<br/>📤 输出: B×L/256×16C]\n    end\n    \n    subgraph \"核心算法处理\"\n        A1[WindowAttention<br/>📥 输入: 窗口特征<br/>🔄 处理: 余弦注意力+CPB<br/>📤 输出: 注意力特征]\n        A2[残差后归一化<br/>📥 输入: 注意力输出<br/>🔄 处理: LN+残差连接<br/>📤 输出: 稳定特征]\n        A3[对数间隔CPB<br/>📥 输入: 相对坐标<br/>🔄 处理: 对数变换+MLP<br/>📤 输出: 位置偏差]\n    end\n    \n    subgraph \"输出结果\"\n        R1[分类结果<br/>B×num_classes]\n        R2[特征图<br/>B×C×H×W]\n        R3[激活值分布]\n    end\n    \n    D1 --> P1\n    P1 --> P2\n    P2 --> P3\n    P3 --> P4\n    P4 --> P5\n    P2 --> A1\n    A1 --> A2\n    A3 --> A1\n    P5 --> R1\n    P5 --> R2\n    P5 --> R3\n    \n    classDef dataNode fill:#e1f5fe\n    classDef processNode fill:#f3e5f5\n    classDef algorithmNode fill:#fff3e0\n    classDef resultNode fill:#e8f5e8\n    \n    class D1,D2,D3 dataNode\n    class P1,P2,P3,P4,P5 processNode\n    class A1,A2,A3 algorithmNode\n    class R1,R2,R3 resultNode\n```\n\n## 3. 核心算法实现分析\n\n### 3.1 残差后归一化 (Res-Post-Norm)\n\n**算法名称**: Res-Post-Norm 稳定化机制\n**实现位置**: `models/swin_transformer_v2.py:301, 304`\n\n**输入参数**:\n- `x`: 输入特征张量，形状为 `B×H×W×C`\n- `shortcut`: 残差连接的原始输入\n\n**处理步骤**:\n```python\n# 1. 残差连接后进行LayerNorm\nx = shortcut + self.drop_path(self.norm1(x))\n\n# 2. FFN层也采用后归一化\nx = x + self.drop_path(self.norm2(self.mlp(x)))\n```\n\n**状态演化示例**:\n假设输入特征 `x` 的激活值在 [-2.0, 3.0] 范围内:\n- 初始状态: `x = [-2.0, 0.5, 1.2, 3.0]`\n- 第一步（注意力输出）: `attn_out = [-1.8, 0.3, 1.5, 2.8]`\n- LayerNorm归一化: `norm1_out = [-1.2, 0.1, 0.8, 1.3]` (均值为0，方差为1)\n- 残差连接: `res_out = [-3.2, 0.6, 2.0, 4.3]` (原始输入 + 归一化输出)\n\n**输出格式**: 形状不变的归一化特征张量\n\n**关键实现细节**:\n- 在 `SwinTransformerBlock.__init__()` 中初始化两个LayerNorm层\n- 在 `_init_respostnorm()` 方法中将所有LayerNorm的权重和偏置初始化为0\n- 对于大模型(H/G)，每6个Transformer块在主分支额外增加LayerNorm层\n\n### 3.2 缩放余弦注意力 (Scaled Cosine Attention)\n\n**算法名称**: 缩放余弦注意力计算\n**实现位置**: `models/swin_transformer_v2.py:154-157`\n\n**输入参数**:\n- `q`: 查询向量，形状为 `B×num_heads×N×head_dim`\n- `k`: 键向量，形状为 `B×num_heads×N×head_dim`\n- `logit_scale`: 可学习的温度参数，形状为 `num_heads×1×1`\n\n**处理步骤**:\n```python\n# 1. 计算余弦相似度\nattn = (F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1))\n\n# 2. 应用可学习缩放因子\nlogit_scale = torch.clamp(self.logit_scale, max=torch.log(torch.tensor(1. / 0.01))).exp()\nattn = attn * logit_scale\n\n# 3. 添加位置偏差\nrelative_position_bias = 16 * torch.sigmoid(relative_position_bias)\nattn = attn + relative_position_bias.unsqueeze(0)\n```\n\n**状态演化示例**:\n假设查询向量 `q = [1.0, 2.0, 3.0]`，键向量 `k = [0.5, 1.5, 2.5]`:\n- L2归一化: `q_norm = [0.27, 0.53, 0.80]`, `k_norm = [0.16, 0.48, 0.80]`\n- 余弦相似度: `cos_sim = q_norm @ k_norm^T = 0.88`\n- 缩放因子: `τ = 2.0` (可学习参数)\n- 最终注意力值: `attn = 0.88 × 2.0 = 1.76`\n\n**输出格式**: 注意力权重矩阵，形状为 `B×num_heads×N×N`\n\n**关键实现细节**:\n- 使用 `F.normalize()` 进行L2归一化\n- 温度参数 `logit_scale` 初始化为 `log(10)`，限制最大值为 `log(1/0.01)`\n- 通过sigmoid函数将位置偏差限制在合理范围内\n\n### 3.3 对数间隔连续位置偏差 (Log-Spaced CPB)\n\n**算法名称**: 对数间隔连续位置偏差生成\n**实现位置**: `models/swin_transformer_v2.py:98-111`\n\n**输入参数**:\n- `window_size`: 窗口大小，如 `(7, 7)`\n- `pretrained_window_size`: 预训练窗口大小\n\n**处理步骤**:\n```python\n# 1. 生成相对坐标表\nrelative_coords_h = torch.arange(-(self.window_size[0] - 1), self.window_size[0], dtype=torch.float32)\nrelative_coords_w = torch.arange(-(self.window_size[1] - 1), self.window_size[1], dtype=torch.float32)\nrelative_coords_table = torch.stack(torch.meshgrid([relative_coords_h, relative_coords_w]))\n\n# 2. 坐标归一化\nrelative_coords_table[:, :, :, 0] /= (self.window_size[0] - 1)\nrelative_coords_table[:, :, :, 1] /= (self.window_size[1] - 1)\nrelative_coords_table *= 8\n\n# 3. 对数间隔变换\nrelative_coords_table = torch.sign(relative_coords_table) * torch.log2(\n    torch.abs(relative_coords_table) + 1.0) / np.log2(8)\n\n# 4. 通过MLP生成位置偏差\nrelative_position_bias_table = self.cpb_mlp(self.relative_coords_table)\n```\n\n**状态演化示例**:\n从8×8窗口迁移到16×16窗口:\n- 线性坐标: `[-7, 7]` → `[-15, 15]`，外推比例 1.14×\n- 对数坐标: `[-2.079, 2.079]` → `[-2.773, 2.773]`，外推比例 0.33×\n- MLP处理: `log_coords = [-2.079, -1.386, 0.0, 1.386, 2.079]`\n- 位置偏差: `bias = mlp(log_coords) = [-0.3, -0.1, 0.0, 0.1, 0.3]`\n\n**输出格式**: 位置偏差张量，形状为 `(2*Wh-1)×(2*Ww-1)×num_heads`\n\n**关键实现细节**:\n- 使用2层MLP网络生成连续位置偏差\n- 对数变换减少不同窗口大小间的外推需求\n- 预训练和微调时共享相同的MLP权重\n\n## 4. 模块结构文档\n\n### 4.1 PatchEmbed模块\n\n**模块名称**: Patch Embedding Layer\n**代码位置**: `models/swin_transformer_v2.py:455-501`\n\n**目的**: 将输入图像分割成不重叠的补丁并转换为嵌入向量\n\n**输入**: \n- 图像张量: `B×C×H×W`\n- 图像大小: `img_size` (默认224)\n- 补丁大小: `patch_size` (默认4)\n\n**输出**: \n- 嵌入特征: `B×(H/patch_size×W/patch_size)×embed_dim`\n\n**依赖**: 无\n\n**实现要点**:\n- 使用4×4卷积核进行特征提取\n- 支持可选的LayerNorm归一化\n- 计算并存储补丁分辨率信息\n\n### 4.2 WindowAttention模块\n\n**模块名称**: Window-based Multi-head Self Attention\n**代码位置**: `models/swin_transformer_v2.py:67-197`\n\n**目的**: 在局部窗口内执行基于余弦相似度的自注意力计算\n\n**输入**:\n- 窗口特征: `(num_windows×B)×window_size×window_size×C`\n- 注意力掩码: `(num_windows, window_size×window_size, window_size×window_size)`\n\n**输出**:\n- 注意力特征: `(num_windows×B)×window_size×window_size×C`\n\n**依赖**: 无\n\n**实现要点**:\n- 实现缩放余弦注意力机制\n- 集成连续位置偏差生成\n- 支持移位窗口注意力(SW-MSA)\n\n### 4.3 SwinTransformerBlock模块\n\n**模块名称**: Swin Transformer 基础块\n**代码位置**: `models/swin_transformer_v2.py:199-325`\n\n**目的**: 实现完整的Transformer块，包含注意力和前馈网络\n\n**输入**:\n- 输入特征: `B×(H×W)×C`\n- 输入分辨率: `(H, W)`\n\n**输出**:\n- 输出特征: `B×(H×W)×C`\n\n**依赖**: WindowAttention, Mlp, LayerNorm\n\n**实现要点**:\n- 采用残差后归一化配置\n- 支持窗口移位机制\n- 集成随机深度(drop path)\n\n### 4.4 BasicLayer模块\n\n**模块名称**: Swin Transformer 层级\n**代码位置**: `models/swin_transformer_v2.py:376-445`\n\n**目的**: 构建一个完整的Transformer层级，包含多个基础块\n\n**输入**:\n- 输入特征: `B×L×C`\n- 输入分辨率: `(H, W)`\n\n**输出**:\n- 输出特征: `B×(L/4)×2C` (如果包含下采样)\n\n**依赖**: SwinTransformerBlock, PatchMerging\n\n**实现要点**:\n- 交替使用常规和移位窗口\n- 可选的梯度检查点功能\n- 集成补丁合并下采样\n\n### 4.5 SimMIM预训练模块\n\n**模块名称**: Swin Transformer for SimMIM\n**代码位置**: `models/simmim.py:79-100`\n\n**目的**: 为自监督预训练提供掩码图像建模功能\n\n**输入**:\n- 输入图像: `B×C×H×W`\n- 二进制掩码: `B×H×W`\n\n**输出**:\n- 重构特征: `B×C×H×W`\n\n**依赖**: SwinTransformerV2\n\n**实现要点**:\n- 集成可学习的掩码token\n- 支持掩码图像建模预训练\n- 保持原始SwinV2架构不变\n\n## 5. 内存优化实现\n\n### 5.1 激活检查点 (Activation Checkpointing)\n\n**实现位置**: `models/swin_transformer_v2.py:428-431`\n```python\nif self.use_checkpoint:\n    x = checkpoint.checkpoint(blk, x)\nelse:\n    x = blk(x)\n```\n\n**效果**: 减少30%内存消耗，降低约30%训练速度\n\n### 5.2 Zero-Redundancy Optimizer (ZeRO)\n\n**实现位置**: 通过DeepSpeed框架集成，在配置文件中启用\n**效果**: 将参数和优化器状态分布到多个GPU，显著减少内存需求\n\n### 5.3 顺序自注意力计算\n\n**实现方式**: 通过配置选项在前两个阶段启用\n**效果**: 降低计算过程内存占用，支持高分辨率图像处理\n\n## 6. 模型配置与扩展\n\n### 6.1 标准配置\n- **SwinV2-T**: embed_dim=96, depths=[2,2,6,2], num_heads=[3,6,12,24]\n- **SwinV2-S**: embed_dim=96, depths=[2,2,18,2], num_heads=[3,6,12,24]\n- **SwinV2-B**: embed_dim=128, depths=[2,2,18,2], num_heads=[4,8,16,32]\n- **SwinV2-L**: embed_dim=192, depths=[2,2,18,2], num_heads=[6,12,24,48]\n\n### 6.2 大规模配置\n- **SwinV2-H** (658M参数): embed_dim=352, depths=[2,2,18,2], num_heads=[11,22,44,88]\n- **SwinV2-G** (3B参数): embed_dim=512, depths=[2,2,42,4], num_heads=[16,32,64,128]\n\n## 7. 训练与推理流程\n\n### 7.1 自监督预训练流程\n1. **第一阶段**: SimMIM自监督预训练 (`main_simmim_pt.py`)\n2. **第二阶段**: 有监督预训练 (`main_simmim_ft.py`)\n\n### 7.2 关键训练参数\n- 批大小: 9216 (多GPU分布式)\n- 优化器: AdamW\n- 学习率调度: 余弦衰减\n- 权重衰减: 0.1\n- 梯度裁剪: 最大范数100.0\n\n### 7.3 数据增强策略\n- 随机调整大小裁剪: 比例范围[0.67,1]，纵横比[3/4,4/3]\n- 随机水平翻转\n- 颜色归一化\n- RandAugment (有监督阶段)\n\n## 8. 性能优化特性\n\n### 8.1 推理优化\n- 位置偏差预计算并存储为模型参数\n- 支持不同窗口大小的零样本迁移\n- 窗口处理CUDA核优化 (`kernels/window_process/`)\n\n### 8.2 内存效率\n- 梯度检查点减少激活值存储\n- 分布式训练支持\n- 混合精度训练 (AMP)\n\n### 8.3 扩展性\n- 支持不同图像分辨率的输入\n- 可配置的模型深度和宽度\n- 模块化设计便于扩展\n\n## 9. 与论文对应关系\n\n| 论文概念 | 代码实现 | 文件位置 |\n|---------|---------|---------|\n| Res-Post-Norm | `norm1(x)` 在残差连接之后 | swin_transformer_v2.py:301 |\n| Scaled Cosine Attention | `F.normalize(q) @ F.normalize(k)` | swin_transformer_v2.py:155 |\n| Log-Spaced CPB | `torch.log2(abs(coords) + 1.0)` | swin_transformer_v2.py:110-111 |\n| SimMIM预训练 | `SwinTransformerV2ForSimMIM` | simmim.py:79 |\n| ZeRO优化 | DeepSpeed配置 | 配置文件 |\n| 激活检查点 | `use_checkpoint=True` | swin_transformer_v2.py:428 |\n\n这个分析展示了Swin Transformer V2的完整实现细节，从核心算法创新到工程优化，为理解和使用该模型提供了全面的技术参考。",
        "maxLength": 50000,
        "icon": "core/workflow/inputType/input"
      },
      {
        "key": "ocN5KV4O",
        "label": "outside_knowledge",
        "type": "input",
        "description": "",
        "required": true,
        "valueType": "string",
        "defaultValue": "[\"https://www.cnblogs.com/dan-baishucaizi/p/16375798.html\", \"https://www.cnblogs.com/tian777/p/17935080.html\", \"https://hub.baai.ac.cn/view/12078\", \"https://blog.csdn.net/qq_39698985/article/details/148760031\", \"https://blog.csdn.net/qq_42957563/article/details/138293801\", \"https://blog.csdn.net/gitblog_00305/article/details/151175570\", \"https://blog.csdn.net/weixin_54171657/article/details/144453955\"]",
        "maxLength": 50000,
        "icon": "core/workflow/inputType/input"
      }
    ],
    "scheduledTriggerConfig": {
      "cronString": "",
      "timezone": "Asia/Shanghai",
      "defaultPrompt": ""
    },
    "_id": "68c5600a5749f574d28424ee"
  }
}